# hadoop

**概念解释**

hadoop有很多工作项目，辅助计算的，负责存储的，负责文件调度，所以是一个生态

**`一般是指：Hadoop HDFS分布式存储系统， Hadoop MapReduce分布式计算系统， Hadoop YARN分布式资源调度系统， Apache Hive 分布式SQL引擎， Sqoop数据采集工具， 这些在离线计算方面的Apache体系。`**

## 学习笔记 : 

### 名词简介

大数据生态圈：

![image-20230103145639198](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20230103145639198.png)

`分布式和集群的概念`
相同点: 都是由多台机器组成的 ; 
不同点 : 分布式 多台机器 , 每台机器部署不同的组件;
集群: 多台机器 , 每台机器部署相同的组件;

`Hadoop集群概述`：

![image-20230103145826282](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20230103145826282.png)



`Hadoop安装包目录结构介绍`

![image-20230103150107947](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20230103150107947.png)

`Tips`：

![image-20230103150318958](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20230103150318958.png)

`数据与元数据`：

`数据`
指存储的内容本身，比如文件、视频、图片等，这些**数据底层最终是存储在磁盘**等存储介质上的，一般**用户无需关心**，只需要基于目录树进行增删改查即可，实际针对数据的操作由文件系统完成。

 `元数据`
元数据（metadata）又称之为解释性数据，记录数据的数据；
**文件系统元数据**一般指___文件大小、最后修改时间、底层存储位置、属性、所属用户、权限等信息。(Important)___

### HDFS

**![image-20230103151918717](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20230103151918717.png)**

- HDFS（Hadoop Distributed File System ），意为：**Hadoop分布式文件系统**。

- 是Apache Hadoop核心组件之一，作为**大数据生态圈最底层**的分布式存储服务而存在。也可以说大数据首先要解决的问题就是海量数据的存储

- HDFS主要是**解决大数据如何存储问题的**。分布式意味着是HDFS是**横跨在多台计算机**上的存储系统。

- HDFS是一种能够在普通硬件上运行的分布式文件系统，它是**高度容错**的，适应于具有大数据集的应用程序，它非常适于存储大型数据 (比如 TB 和 PB)。

- HDFS使用多台计算机存储文件, 并且提供**统一的访问接口**, 像是访问一个普通文件系统一样使用分布式文件系统。

#### 特点

##### （1）主从架构

- HDFS集群是标准的master/slave主从架构集群。

- 一般一个HDFS集群是有一个Namenode和一定数目的Datanode组成。

- Namenode是HDFS主节点，Datanode是HDFS从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。

- 官方架构图中是一主五从模式，其中五个从角色位于两个机架（Rack）的不同服务器上

  ##### （2）分块

- HDFS中的文件在物理上是分块存储（block）的，默认大小是128M（134217728），不足128M则本身就是一块。

- 块的大小可以通过配置参数来规定，参数位于hdfs-default.xml中：dfs.blocksize。

  ##### （3）副本机制

  - 文件的所有block都会有副本。副本系数可以在文件创建的时候指定，也可以在之后通过命令改变。

  - 副本数由参数dfs.replication控制，默认值是3，也就是会额外再复制2份，连同本身总共3份副本.

##### （4）元数据管理

在HDFS中，**Namenode管理的元数据**具有两种类型:

- 文件自身属性信息

  ​	文件名称、权限，修改时间，文件大小，复制因子，数据块

- 文件块位置映射信息

  ​    记录文件块和DataNode之间的映射信息，即哪个块位于哪个节点上。

  ##### （5）namespace

- HDFS支持传统的层次型文件组织结构。用户可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。

- Namenode负责维护文件系统的namespace名称空间，任何对文件系统名称空间或属性的修改都将被Namenode记录下来。

- HDFS会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如:hdfs://namenode:port/dira/dir-b/dir-c/file.data.

##### （6）数据块存储

- 文件的各个block的具体存储管理由DataNode节点
- 每一个block都可以在多个DataNode上存储。











操作系统四大分类 : 
桌面操作系统 : 图形化界面 Mac OS , Windows , Linux
服务器操作系统 : 一般指的是安装在大型计算机上的操作系统 ,
嵌入式操作系统 : 完全嵌入受控制器件内部 , 为特定应用而设计的专用计算机系统;
移动设备操作系统 : 主要应用在智能手机 , 平板, 等智能设备上 ;
操作系统的核心部分简称内核;

ssh协议 :
是一种网络安全协议 , 为远程登录会话和其他网络服务提供安全性的协议.
在Linux系统中 , ssh主要用途由 : 用户加密实现远程登录, 服务器之间的免密登录,
ssh协议默认使用rsa算法实现非对称加密 ,需要两个密钥 : 公钥和私钥
公钥和私钥是一对 , 如果使用公钥对数据进行加密 ,只用用对应的私钥才能解密 ;
ssh加密实现远程登录的步骤 : 

1. 客户端向服务器端发起ssh请求 ;
2. 服务器接收到请求 , 发送公钥给客户端 ;
3. 客户端输入用户名和密码 进行公钥加密 , 回传给客户端 ;
4. 服务器通过私钥解密得到用户名和密码和本地进行对比 ,验证成功 , 允许登录 , 否则再次验证 ; 

具体的操作步骤：

```shell
ssh免密登录


Linux版本：CentOS7.9


1. 修改配置文件`/etc/ssh/sshd_config`

  # 1. 打开配置文件

  vim /etc/ssh/sshd_config

  

  # 2. 在配置文件中配置如下内容

  PubkeyAuthentication yes  # 43行，将注释取消即可

  AuthorizedKeysFile   .ssh/authorized_keys # 47行，配置文件默认内容，无需修改

  

  # 3. 退出保存





2. 重启ssh服务

  systemctl restart sshd





3. 生成公钥文件id_rsa.pub

  # 1. 在用户目录下创建一个文件夹.ssh，root用户如下：

  mkdir /root/.ssh

  # 2. 进入.ssh目录

  cd /root/.ssh

  # 3. 生成公钥文件id_rsa.pub

  ssh-keygen -t rsa   # 一路enter，不要自定义名称

  # 4. 在.ssh目录下创建authorized_keys文件（有可以不创建）

  touch authorized_keys

  # 5. 文件授权

  chmod 700 /root/.ssh

  chmod 600 authorized_keys





4. 复制id_rsa.pub内容到authorized_keys文件中

  # .ssh目录下,通过重定向将公钥文件内容复制到authorized_keys文件中

  cat id_rsa.pub > authorized_keys





5. 复制配置文件到集群中的其它主机



  # 例：复制配置文件到hadoop101

  scp /etc/ssh/sshd_config       root@hadoop101:/etc/ssh/

  scp -r /root/.ssh root@hadoop101:/root

  

  # 在hadoop101中执行如下操作

  # 重启ssh服务

  systemctl restart sshd

  # 在.ssh目录下生成公钥文件

  cd /root/.ssh

  ssh-keygen -t rsa   # 一路enter，不要自定义名称。Override？yes

  # .ssh目录下,通过追加重定向将公钥文件内容复制到authorized_keys文件中

  cat id_rsa.pub >> authorized_keys

  

  # 在其它主机上重复如上操作，将所有主机的公钥内容都复制到authorized_keys文件中





6. 将包含所有主机公钥内容的authorized_keys文件分发到集群中所有主机上

  # 方式1. 调用自定义分发脚本

  xsync /root/.ssh/authorized_keys

  

  # 方式2. 使用scp一台主机一台主机的复制，如：复制到hadoop100主机

scp /root/.ssh/authorized_keys root@hadoop100:/root/ssh





7. 配置完毕，可以使用ssh命令测试是否成功。

  # 在hadoop100主机上连接hadoop101

  ssh hadoop101
```

`Linux 操作系统常用命令` : 
ls : 查看当前目录下所有文件和文件夹的名称 ; 
cd : 切换到指定目录 ; 
mkdir : 在指定目录下创建一个文件夹 ; 
rm : 用于删除文档 ;

### MapReduce

MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 [Hadoop](https://so.csdn.net/so/search?q=Hadoop&spm=1001.2101.3001.7020) 的数据分析应用”的核心框架。
MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。

![image-20221230142317403](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20221230142317403.png)











### Hive

Hive是基于Hadoop的一个`数据仓库工具`，可以将`结构化的数据文件映射为一张表`，并提供`类SQL`查询功能。

__本质是：将HQL转化成MapReduce程序__

![image-20221230143116468](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20221230143116468.png)

```特点```

​	1）Hive处理的数据存储在HDFS

​	2）Hive分析数据底层的实现是MapReduce

​	3）执行程序运行在Yarn上

Hive的优缺点

 	优点
 				操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。

​				避免了去写MapReduce，减少开发人员的学习成本。

​				Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。

​				Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。

​				Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。

缺点
			1．Hive的HQL表达能力有限

​						（1）迭代式算法无法表达

​						（2）数据挖掘方面不擅长

​      	 2．Hive的效率比较低

​						（1）Hive自动生成的MapReduce作业，通常情况下不够智能化

​						（2）Hive调优比较困难，粒度较粗

`Hive架构原理`

![image-20221230143646279](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20221230143646279.png)

1．用户接口：Client

​		CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）

2．元数据：Metastore

​		元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型		（是否是外部表）、表的数据所在目录等；

​		默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore

3．Hadoop

​		使用HDFS进行存储，使用MapReduce进行计算。

4．驱动器：Driver

​		（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库			完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。

​		（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。

​		（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。

​		（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是			MR/Spark。

`Hive运行机制示意图`

![image-20221230143845118](https://chart-beded.oss-cn-shenzhen.aliyuncs.com/img/image-20221230143845118.png)

`总结`

Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。

